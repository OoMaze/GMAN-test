{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libriary\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "file_path = r'C:\\Users\\Administrator\\Desktop\\GMAN-PyTorch-master'\n",
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "# log string\n",
    "def log_string(log, string):\n",
    "    log.write(string + '\\n')\n",
    "    log.flush()\n",
    "    print(string)\n",
    "\n",
    "\n",
    "# metric\n",
    "def metric(pred, label):\n",
    "    mask = torch.ne(label, 0)\n",
    "    mask = mask.type(torch.float32)\n",
    "    mask /= torch.mean(mask)\n",
    "    mae = torch.abs(torch.sub(pred, label)).type(torch.float32)\n",
    "    rmse = mae ** 2\n",
    "    mape = mae / label\n",
    "    mae = torch.mean(mae)\n",
    "    rmse = rmse * mask\n",
    "    rmse = torch.sqrt(torch.mean(rmse))\n",
    "    mape = mape * mask\n",
    "    mape = torch.mean(mape)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "\n",
    "def seq2instance(data, num_his, num_pred):\n",
    "    num_step, dims = data.shape\n",
    "    num_sample = num_step - num_his - num_pred + 1\n",
    "    x = torch.zeros(num_sample, num_his, dims)\n",
    "    y = torch.zeros(num_sample, num_pred, dims)\n",
    "    for i in range(num_sample):\n",
    "        x[i] = data[i: i + num_his]\n",
    "        y[i] = data[i + num_his: i + num_his + num_pred]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    # Traffic\n",
    "    df = pd.read_hdf(args.traffic_file)\n",
    "    traffic = torch.from_numpy(df.values)\n",
    "    # train/val/test\n",
    "    num_step = df.shape[0]\n",
    "    train_steps = round(args.train_ratio * num_step)\n",
    "    test_steps = round(args.test_ratio * num_step)\n",
    "    val_steps = num_step - train_steps - test_steps\n",
    "    train = traffic[: train_steps]\n",
    "    val = traffic[train_steps: train_steps + val_steps]\n",
    "    test = traffic[-test_steps:]\n",
    "    # X, Y\n",
    "    trainX, trainY = seq2instance(train, args.num_his, args.num_pred)\n",
    "    valX, valY = seq2instance(val, args.num_his, args.num_pred)\n",
    "    testX, testY = seq2instance(test, args.num_his, args.num_pred)\n",
    "    # normalization\n",
    "    mean, std = torch.mean(trainX), torch.std(trainX)\n",
    "    trainX = (trainX - mean) / std\n",
    "    valX = (valX - mean) / std\n",
    "    testX = (testX - mean) / std\n",
    "\n",
    "    # spatial embedding\n",
    "    with open(args.SE_file, mode='r') as f:\n",
    "        lines = f.readlines()\n",
    "        temp = lines[0].split(' ')\n",
    "        num_vertex, dims = int(temp[0]), int(temp[1])\n",
    "        SE = torch.zeros((num_vertex, dims), dtype=torch.float32)\n",
    "        for line in lines[1:]:\n",
    "            temp = line.split(' ')\n",
    "            index = int(temp[0])\n",
    "            SE[index] = torch.tensor([float(ch) for ch in temp[1:]])\n",
    "\n",
    "    # temporal embedding\n",
    "    time = pd.DatetimeIndex(df.index)\n",
    "    dayofweek = torch.reshape(torch.tensor(time.weekday), (-1, 1))\n",
    "    timeofday = (time.hour * 3600 + time.minute * 60 + time.second) \\\n",
    "                // (5*60)#time.freq.delta.total_seconds()\n",
    "    timeofday = torch.reshape(torch.tensor(timeofday), (-1, 1))\n",
    "    time = torch.cat((dayofweek, timeofday), -1)\n",
    "    # train/val/test\n",
    "    train = time[: train_steps]\n",
    "    val = time[train_steps: train_steps + val_steps]\n",
    "    test = time[-test_steps:]\n",
    "    # shape = (num_sample, num_his + num_pred, 2)\n",
    "    trainTE = seq2instance(train, args.num_his, args.num_pred)\n",
    "    trainTE = torch.cat(trainTE, 1).type(torch.int32)\n",
    "    valTE = seq2instance(val, args.num_his, args.num_pred)\n",
    "    valTE = torch.cat(valTE, 1).type(torch.int32)\n",
    "    testTE = seq2instance(test, args.num_his, args.num_pred)\n",
    "    testTE = torch.cat(testTE, 1).type(torch.int32)\n",
    "\n",
    "    return (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE, testY,\n",
    "            SE, mean, std)\n",
    "\n",
    "\n",
    "# dataset creation\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.len = data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_x[index], self.data_y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# statistic model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# The following function can be replaced by 'loss = torch.nn.L1Loss()  loss_out = loss(pred, target)\n",
    "def mae_loss(pred, label):\n",
    "    mask = torch.ne(label, 0)\n",
    "    mask = mask.type(torch.float32)\n",
    "    mask /= torch.mean(mask)\n",
    "    mask = torch.where(torch.isnan(mask), torch.tensor(0.0), mask)\n",
    "    loss = torch.abs(torch.sub(pred, label))\n",
    "    loss *= mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.tensor(0.0), loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# plot train_val_loss\n",
    "def plot_train_val_loss(train_total_loss, val_total_loss, file_path):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_total_loss) + 1), train_total_loss, c='b', marker='s', label='Train')\n",
    "    plt.plot(range(1, len(val_total_loss) + 1), val_total_loss, c='r', marker='o', label='Validation')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Train loss vs Validation loss')\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "\n",
    "# plot test results\n",
    "def save_test_result(trainPred, trainY, valPred, valY, testPred, testY):\n",
    "    with open('./figure/test_results.txt', 'w+') as f:\n",
    "        for l in (trainPred, trainY, valPred, valY, testPred, testY):\n",
    "            f.write(list(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "class conv2d_(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims, kernel_size, stride=(1, 1),\n",
    "                 padding='SAME', use_bias=True, activation=F.relu,\n",
    "                 bn_decay=None):\n",
    "        super(conv2d_, self).__init__()\n",
    "        self.activation = activation\n",
    "        if padding == 'SAME':\n",
    "            self.padding_size = math.ceil(kernel_size)\n",
    "        else:\n",
    "            self.padding_size = [0, 0]\n",
    "        self.conv = nn.Conv2d(input_dims, output_dims, kernel_size, stride=stride,\n",
    "                              padding=0, bias=use_bias)\n",
    "        self.batch_norm = nn.BatchNorm2d(output_dims, momentum=bn_decay)\n",
    "        torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "\n",
    "        if use_bias:\n",
    "            torch.nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = F.pad(x, ([self.padding_size[1], self.padding_size[1], self.padding_size[0], self.padding_size[0]]))\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        if self.activation is not None:\n",
    "            x = F.relu_(x)\n",
    "        return x.permute(0, 3, 2, 1)\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, input_dims, units, activations, bn_decay, use_bias=True):\n",
    "        super(FC, self).__init__()\n",
    "        if isinstance(units, int):\n",
    "            units = [units]\n",
    "            input_dims = [input_dims]\n",
    "            activations = [activations]\n",
    "        elif isinstance(units, tuple):\n",
    "            units = list(units)\n",
    "            input_dims = list(input_dims)\n",
    "            activations = list(activations)\n",
    "        assert type(units) == list\n",
    "        self.convs = nn.ModuleList([conv2d_(\n",
    "            input_dims=input_dim, output_dims=num_unit, kernel_size=[1, 1], stride=[1, 1],\n",
    "            padding='VALID', use_bias=use_bias, activation=activation,\n",
    "            bn_decay=bn_decay) for input_dim, num_unit, activation in\n",
    "            zip(input_dims, units, activations)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class STEmbedding(nn.Module):\n",
    "    '''\n",
    "    spatio-temporal embedding\n",
    "    SE:     [num_vertex, D]\n",
    "    TE:     [batch_size, num_his + num_pred, 2] (dayofweek, timeofday)\n",
    "    T:      num of time steps in one day\n",
    "    D:      output dims\n",
    "    retrun: [batch_size, num_his + num_pred, num_vertex, D]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, D, bn_decay):\n",
    "        super(STEmbedding, self).__init__()\n",
    "        self.FC_se = FC(\n",
    "            input_dims=[D, D], units=[D, D], activations=[F.relu, None],\n",
    "            bn_decay=bn_decay)\n",
    "\n",
    "        self.FC_te = FC(\n",
    "            input_dims=[295, D], units=[D, D], activations=[F.relu, None],\n",
    "            bn_decay=bn_decay)  # input_dims = time step per day + days per week=288+7=295\n",
    "\n",
    "    def forward(self, SE, TE, T=288):\n",
    "        # spatial embedding\n",
    "        SE = SE.unsqueeze(0).unsqueeze(0)\n",
    "        SE = self.FC_se(SE)\n",
    "        # temporal embedding\n",
    "        dayofweek = torch.empty(TE.shape[0], TE.shape[1], 7)\n",
    "        timeofday = torch.empty(TE.shape[0], TE.shape[1], T)\n",
    "        for i in range(TE.shape[0]):\n",
    "            dayofweek[i] = F.one_hot(TE[..., 0][i].to(torch.int64) % 7, 7)\n",
    "        for j in range(TE.shape[0]):\n",
    "            timeofday[j] = F.one_hot(TE[..., 1][j].to(torch.int64) % 288, T)\n",
    "        TE = torch.cat((dayofweek, timeofday), dim=-1)\n",
    "        TE = TE.unsqueeze(dim=2)\n",
    "        TE = self.FC_te(TE)\n",
    "        del dayofweek, timeofday\n",
    "        return SE + TE\n",
    "\n",
    "\n",
    "class spatialAttention(nn.Module):\n",
    "    '''\n",
    "    spatial attention mechanism\n",
    "    X:      [batch_size, num_step, num_vertex, D]\n",
    "    STE:    [batch_size, num_step, num_vertex, D]\n",
    "    K:      number of attention heads\n",
    "    d:      dimension of each attention outputs\n",
    "    return: [batch_size, num_step, num_vertex, D]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, d, bn_decay):\n",
    "        super(spatialAttention, self).__init__()\n",
    "        D = K * d\n",
    "        self.d = d\n",
    "        self.K = K\n",
    "        self.FC_q = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_k = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_v = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                     bn_decay=bn_decay)\n",
    "\n",
    "    def forward(self, X, STE):\n",
    "        batch_size = X.shape[0]\n",
    "        X = torch.cat((X, STE), dim=-1)\n",
    "        # [batch_size, num_step, num_vertex, K * d]\n",
    "        query = self.FC_q(X)\n",
    "        key = self.FC_k(X)\n",
    "        value = self.FC_v(X)\n",
    "        # [K * batch_size, num_step, num_vertex, d]\n",
    "        query = torch.cat(torch.split(query, self.K, dim=-1), dim=0)\n",
    "        key = torch.cat(torch.split(key, self.K, dim=-1), dim=0)\n",
    "        value = torch.cat(torch.split(value, self.K, dim=-1), dim=0)\n",
    "        # [K * batch_size, num_step, num_vertex, num_vertex]\n",
    "        attention = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention /= (self.d ** 0.5)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # [batch_size, num_step, num_vertex, D]\n",
    "        X = torch.matmul(attention, value)\n",
    "        X = torch.cat(torch.split(X, batch_size, dim=0), dim=-1)  # orginal K, change to batch_size\n",
    "        X = self.FC(X)\n",
    "        del query, key, value, attention\n",
    "        return X\n",
    "\n",
    "\n",
    "class temporalAttention(nn.Module):\n",
    "    '''\n",
    "    temporal attention mechanism\n",
    "    X:      [batch_size, num_step, num_vertex, D]\n",
    "    STE:    [batch_size, num_step, num_vertex, D]\n",
    "    K:      number of attention heads\n",
    "    d:      dimension of each attention outputs\n",
    "    return: [batch_size, num_step, num_vertex, D]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, d, bn_decay, mask=True):\n",
    "        super(temporalAttention, self).__init__()\n",
    "        D = K * d\n",
    "        self.d = d\n",
    "        self.K = K\n",
    "        self.mask = mask\n",
    "        self.FC_q = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_k = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_v = FC(input_dims=2 * D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                     bn_decay=bn_decay)\n",
    "\n",
    "    def forward(self, X, STE):\n",
    "        batch_size_ = X.shape[0]\n",
    "        X = torch.cat((X, STE), dim=-1)\n",
    "        # [batch_size, num_step, num_vertex, K * d]\n",
    "        query = self.FC_q(X)\n",
    "        key = self.FC_k(X)\n",
    "        value = self.FC_v(X)\n",
    "        # [K * batch_size, num_step, num_vertex, d]\n",
    "        query = torch.cat(torch.split(query, self.K, dim=-1), dim=0)\n",
    "        key = torch.cat(torch.split(key, self.K, dim=-1), dim=0)\n",
    "        value = torch.cat(torch.split(value, self.K, dim=-1), dim=0)\n",
    "        # query: [K * batch_size, num_vertex, num_step, d]\n",
    "        # key:   [K * batch_size, num_vertex, d, num_step]\n",
    "        # value: [K * batch_size, num_vertex, num_step, d]\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 3, 1)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "        # [K * batch_size, num_vertex, num_step, num_step]\n",
    "        attention = torch.matmul(query, key)\n",
    "        attention /= (self.d ** 0.5)\n",
    "        # mask attention score\n",
    "        if self.mask:\n",
    "            batch_size = X.shape[0]\n",
    "            num_step = X.shape[1]\n",
    "            num_vertex = X.shape[2]\n",
    "            mask = torch.ones(num_step, num_step)\n",
    "            mask = torch.tril(mask)\n",
    "            mask = torch.unsqueeze(torch.unsqueeze(mask, dim=0), dim=0)\n",
    "            mask = mask.repeat(self.K * batch_size, num_vertex, 1, 1)\n",
    "            mask = mask.to(torch.bool)\n",
    "            attention = torch.where(mask, attention, -2 ** 15 + 1)\n",
    "        # softmax\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # [batch_size, num_step, num_vertex, D]\n",
    "        X = torch.matmul(attention, value)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        X = torch.cat(torch.split(X, batch_size_, dim=0), dim=-1)  # orginal K, change to batch_size\n",
    "        X = self.FC(X)\n",
    "        del query, key, value, attention\n",
    "        return X\n",
    "\n",
    "\n",
    "class gatedFusion(nn.Module):\n",
    "    '''\n",
    "    gated fusion\n",
    "    HS:     [batch_size, num_step, num_vertex, D]\n",
    "    HT:     [batch_size, num_step, num_vertex, D]\n",
    "    D:      output dims\n",
    "    return: [batch_size, num_step, num_vertex, D]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, D, bn_decay):\n",
    "        super(gatedFusion, self).__init__()\n",
    "        self.FC_xs = FC(input_dims=D, units=D, activations=None,\n",
    "                        bn_decay=bn_decay, use_bias=False)\n",
    "        self.FC_xt = FC(input_dims=D, units=D, activations=None,\n",
    "                        bn_decay=bn_decay, use_bias=True)\n",
    "        self.FC_h = FC(input_dims=[D, D], units=[D, D], activations=[F.relu, None],\n",
    "                       bn_decay=bn_decay)\n",
    "\n",
    "    def forward(self, HS, HT):\n",
    "        XS = self.FC_xs(HS)\n",
    "        XT = self.FC_xt(HT)\n",
    "        z = torch.sigmoid(torch.add(XS, XT))\n",
    "        H = torch.add(torch.mul(z, HS), torch.mul(1 - z, HT))\n",
    "        H = self.FC_h(H)\n",
    "        del XS, XT, z\n",
    "        return H\n",
    "\n",
    "\n",
    "class STAttBlock(nn.Module):\n",
    "    def __init__(self, K, d, bn_decay, mask=False):\n",
    "        super(STAttBlock, self).__init__()\n",
    "        self.spatialAttention = spatialAttention(K, d, bn_decay)\n",
    "        self.temporalAttention = temporalAttention(K, d, bn_decay, mask=mask)\n",
    "        self.gatedFusion = gatedFusion(K * d, bn_decay)\n",
    "\n",
    "    def forward(self, X, STE):\n",
    "        HS = self.spatialAttention(X, STE)\n",
    "        HT = self.temporalAttention(X, STE)\n",
    "        H = self.gatedFusion(HS, HT)\n",
    "        del HS, HT\n",
    "        return torch.add(X, H)\n",
    "\n",
    "\n",
    "class transformAttention(nn.Module):\n",
    "    '''\n",
    "    transform attention mechanism\n",
    "    X:        [batch_size, num_his, num_vertex, D]\n",
    "    STE_his:  [batch_size, num_his, num_vertex, D]\n",
    "    STE_pred: [batch_size, num_pred, num_vertex, D]\n",
    "    K:        number of attention heads\n",
    "    d:        dimension of each attention outputs\n",
    "    return:   [batch_size, num_pred, num_vertex, D]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, d, bn_decay):\n",
    "        super(transformAttention, self).__init__()\n",
    "        D = K * d\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.FC_q = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_k = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_v = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC = FC(input_dims=D, units=D, activations=F.relu,\n",
    "                     bn_decay=bn_decay)\n",
    "\n",
    "    def forward(self, X, STE_his, STE_pred):\n",
    "        batch_size = X.shape[0]\n",
    "        # [batch_size, num_step, num_vertex, K * d]\n",
    "        query = self.FC_q(STE_pred)\n",
    "        key = self.FC_k(STE_his)\n",
    "        value = self.FC_v(X)\n",
    "        # [K * batch_size, num_step, num_vertex, d]\n",
    "        query = torch.cat(torch.split(query, self.K, dim=-1), dim=0)\n",
    "        key = torch.cat(torch.split(key, self.K, dim=-1), dim=0)\n",
    "        value = torch.cat(torch.split(value, self.K, dim=-1), dim=0)\n",
    "        # query: [K * batch_size, num_vertex, num_pred, d]\n",
    "        # key:   [K * batch_size, num_vertex, d, num_his]\n",
    "        # value: [K * batch_size, num_vertex, num_his, d]\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 3, 1)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "        # [K * batch_size, num_vertex, num_pred, num_his]\n",
    "        attention = torch.matmul(query, key)\n",
    "        attention /= (self.d ** 0.5)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # [batch_size, num_pred, num_vertex, D]\n",
    "        X = torch.matmul(attention, value)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        X = torch.cat(torch.split(X, batch_size, dim=0), dim=-1)\n",
    "        X = self.FC(X)\n",
    "        del query, key, value, attention\n",
    "        return X\n",
    "\n",
    "\n",
    "class GMAN(nn.Module):\n",
    "    '''\n",
    "    GMAN\n",
    "        X       [batch_size, num_his, num_vertx]\n",
    "        TE      [batch_size, num_his + num_pred, 2] (time-of-day, day-of-week)\n",
    "        SE      [num_vertex, K * d]\n",
    "        num_his number of history steps\n",
    "        num_pred umber of prediction steps\n",
    "        T       one day is divided into T steps\n",
    "        L       number of STAtt blocks in the encoder/decoder\n",
    "        K       number of attention heads\n",
    "        d       dimension of each attention head outputs\n",
    "        return  [batch_size, num_pred, num_vertex]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, SE, args, bn_decay):\n",
    "        super(GMAN, self).__init__()\n",
    "        L = args.L\n",
    "        K = args.K\n",
    "        d = args.d\n",
    "        D = K * d\n",
    "        self.num_his = args.num_his\n",
    "        self.SE = SE\n",
    "        self.STEmbedding = STEmbedding(D, bn_decay)\n",
    "        self.STAttBlock_1 = nn.ModuleList([STAttBlock(K, d, bn_decay) for _ in range(L)])\n",
    "        self.STAttBlock_2 = nn.ModuleList([STAttBlock(K, d, bn_decay) for _ in range(L)])\n",
    "        self.transformAttention = transformAttention(K, d, bn_decay)\n",
    "        self.FC_1 = FC(input_dims=[1, D], units=[D, D], activations=[F.relu, None],\n",
    "                       bn_decay=bn_decay)\n",
    "        self.FC_2 = FC(input_dims=[D, D], units=[D, 1], activations=[F.relu, None],\n",
    "                       bn_decay=bn_decay)\n",
    "\n",
    "    def forward(self, X, TE):\n",
    "\n",
    "        # input\n",
    "        X = torch.unsqueeze(X, -1)\n",
    "        X = self.FC_1(X)\n",
    "        # STE\n",
    "        STE = self.STEmbedding(self.SE, TE)\n",
    "        STE_his = STE[:, :self.num_his]\n",
    "        STE_pred = STE[:, self.num_his:]\n",
    "        # encoder\n",
    "        for net in self.STAttBlock_1:\n",
    "            X = net(X, STE_his)\n",
    "        # transAtt\n",
    "        X = self.transformAttention(X, STE_his, STE_pred)\n",
    "        # decoder\n",
    "        for net in self.STAttBlock_2:\n",
    "            X = net(X, STE_pred)\n",
    "        # output\n",
    "        X = self.FC_2(X)\n",
    "        del STE, STE_his, STE_pred\n",
    "        return torch.squeeze(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(model, args, log, loss_criterion, optimizer, scheduler):\n",
    "\n",
    "    (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE,\n",
    "     testY, SE, mean, std) = load_data(args)\n",
    "\n",
    "    num_train, _, num_vertex = trainX.shape\n",
    "    log_string(log, '**** training model ****')\n",
    "    num_val = valX.shape[0]\n",
    "    train_num_batch = math.ceil(num_train / args.batch_size)\n",
    "    val_num_batch = math.ceil(num_val / args.batch_size)\n",
    "\n",
    "    wait = 0\n",
    "    val_loss_min = float('inf')\n",
    "    best_model_wts = None\n",
    "    train_total_loss = []\n",
    "    val_total_loss = []\n",
    "\n",
    "    # Train & validation\n",
    "    for epoch in range(args.max_epoch):\n",
    "        if wait >= args.patience:\n",
    "            log_string(log, f'early stop at epoch: {epoch:04d}')\n",
    "            break\n",
    "        # shuffle\n",
    "        permutation = torch.randperm(num_train)\n",
    "        trainX = trainX[permutation]\n",
    "        trainTE = trainTE[permutation]\n",
    "        trainY = trainY[permutation]\n",
    "        # train\n",
    "        start_train = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx in range(train_num_batch):\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min(num_train, (batch_idx + 1) * args.batch_size)\n",
    "            X = trainX[start_idx: end_idx]\n",
    "            TE = trainTE[start_idx: end_idx]\n",
    "            label = trainY[start_idx: end_idx]\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X, TE)\n",
    "            pred = pred * std + mean\n",
    "            loss_batch = loss_criterion(pred, label)\n",
    "            train_loss += float(loss_batch) * (end_idx - start_idx)\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            #if torch.cuda.is_available():\n",
    "            #torch.cuda.empty_cache()\n",
    "                \n",
    "            if (batch_idx+1) % 5 == 0:\n",
    "                print(f'Training batch: {batch_idx+1} in epoch:{epoch}, training batch loss:{loss_batch:.4f}')\n",
    "            del X, TE, label, pred, loss_batch\n",
    "        train_loss /= num_train\n",
    "        train_total_loss.append(train_loss)\n",
    "        end_train = time.time()\n",
    "\n",
    "        # val loss\n",
    "        start_val = time.time()\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx in range(val_num_batch):\n",
    "                start_idx = batch_idx * args.batch_size\n",
    "                end_idx = min(num_val, (batch_idx + 1) * args.batch_size)\n",
    "                X = valX[start_idx: end_idx]\n",
    "                TE = valTE[start_idx: end_idx]\n",
    "                label = valY[start_idx: end_idx]\n",
    "                pred = model(X, TE)\n",
    "                pred = pred * std + mean\n",
    "                loss_batch = loss_criterion(pred, label)\n",
    "                val_loss += loss_batch * (end_idx - start_idx)\n",
    "                del X, TE, label, pred, loss_batch\n",
    "        val_loss /= num_val\n",
    "        val_total_loss.append(val_loss)\n",
    "        end_val = time.time()\n",
    "        log_string(\n",
    "            log,\n",
    "            '%s | epoch: %04d/%d, training time: %.1fs, inference time: %.1fs' %\n",
    "            (datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch + 1,\n",
    "             args.max_epoch, end_train - start_train, end_val - start_val))\n",
    "        log_string(\n",
    "            log, f'train loss: {train_loss:.4f}, val_loss: {val_loss:.4f}')\n",
    "        if val_loss <= val_loss_min:\n",
    "            log_string(\n",
    "                log,\n",
    "                f'val loss decrease from {val_loss_min:.4f} to {val_loss:.4f}, saving model to {args.model_file}')\n",
    "            wait = 0\n",
    "            val_loss_min = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, args.model_file)\n",
    "    log_string(log, f'Training and validation are completed, and model has been stored as {args.model_file}')\n",
    "    return train_total_loss, val_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "def test(args, log):\n",
    "    (trainX, trainTE, trainY, valX, valTE, valY, testX, testTE,\n",
    "     testY, SE, mean, std) = load_data(args)\n",
    "    num_train, _, num_vertex = trainX.shape\n",
    "    num_val = valX.shape[0]\n",
    "    num_test = testX.shape[0]\n",
    "    train_num_batch = math.ceil(num_train / args.batch_size)\n",
    "    val_num_batch = math.ceil(num_val / args.batch_size)\n",
    "    test_num_batch = math.ceil(num_test / args.batch_size)\n",
    "\n",
    "    model = torch.load(args.model_file)\n",
    "\n",
    "    # test model\n",
    "    log_string(log, '**** testing model ****')\n",
    "    log_string(log, 'loading model from %s' % args.model_file)\n",
    "    model = torch.load(args.model_file)\n",
    "    log_string(log, 'model restored!')\n",
    "    log_string(log, 'evaluating...')\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        trainPred = []\n",
    "        for batch_idx in range(train_num_batch):\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min(num_train, (batch_idx + 1) * args.batch_size)\n",
    "            X = trainX[start_idx: end_idx]\n",
    "            TE = trainTE[start_idx: end_idx]\n",
    "            pred_batch = model(X, TE)\n",
    "            trainPred.append(pred_batch.detach().clone())\n",
    "            del X, TE, pred_batch\n",
    "        trainPred = torch.from_numpy(np.concatenate(trainPred, axis=0))\n",
    "        trainPred = trainPred * std + mean\n",
    "\n",
    "        valPred = []\n",
    "        for batch_idx in range(val_num_batch):\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min(num_val, (batch_idx + 1) * args.batch_size)\n",
    "            X = valX[start_idx: end_idx]\n",
    "            TE = valTE[start_idx: end_idx]\n",
    "            pred_batch = model(X, TE)\n",
    "            valPred.append(pred_batch.detach().clone())\n",
    "            del X, TE, pred_batch\n",
    "        valPred = torch.from_numpy(np.concatenate(valPred, axis=0))\n",
    "        valPred = valPred * std + mean\n",
    "\n",
    "        testPred = []\n",
    "        start_test = time.time()\n",
    "        for batch_idx in range(test_num_batch):\n",
    "            start_idx = batch_idx * args.batch_size\n",
    "            end_idx = min(num_test, (batch_idx + 1) * args.batch_size)\n",
    "            X = testX[start_idx: end_idx]\n",
    "            TE = testTE[start_idx: end_idx]\n",
    "            pred_batch = model(X, TE)\n",
    "            testPred.append(pred_batch.detach().clone())\n",
    "            del X, TE, pred_batch\n",
    "        testPred = torch.from_numpy(np.concatenate(testPred, axis=0))\n",
    "        testPred = testPred* std + mean\n",
    "    end_test = time.time()\n",
    "    train_mae, train_rmse, train_mape = metric(trainPred, trainY)\n",
    "    val_mae, val_rmse, val_mape = metric(valPred, valY)\n",
    "    test_mae, test_rmse, test_mape = metric(testPred, testY)\n",
    "    log_string(log, 'testing time: %.1fs' % (end_test - start_test))\n",
    "    log_string(log, '                MAE\\t\\tRMSE\\t\\tMAPE')\n",
    "    log_string(log, 'train            %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "               (train_mae, train_rmse, train_mape * 100))\n",
    "    log_string(log, 'val              %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "               (val_mae, val_rmse, val_mape * 100))\n",
    "    log_string(log, 'test             %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "               (test_mae, test_rmse, test_mape * 100))\n",
    "    log_string(log, 'performance in each prediction step')\n",
    "    MAE, RMSE, MAPE = [], [], []\n",
    "    for step in range(args.num_pred):\n",
    "        mae, rmse, mape = metric(testPred[:, step], testY[:, step])\n",
    "        MAE.append(mae)\n",
    "        RMSE.append(rmse)\n",
    "        MAPE.append(mape)\n",
    "        log_string(log, 'step: %02d         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "                   (step + 1, mae, rmse, mape * 100))\n",
    "    average_mae = np.mean(MAE)\n",
    "    average_rmse = np.mean(RMSE)\n",
    "    average_mape = np.mean(MAPE)\n",
    "    log_string(\n",
    "        log, 'average:         %.2f\\t\\t%.2f\\t\\t%.2f%%' %\n",
    "             (average_mae, average_rmse, average_mape * 100))\n",
    "    return trainPred, valPred, testPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--time_slot', type=int, default=5,help='a time step is 5 mins')\n",
    "parser.add_argument('--num_his', type=int, default=12,help='history steps')\n",
    "parser.add_argument('--num_pred', type=int, default=12,help='prediction steps')\n",
    "parser.add_argument('--L', type=int, default=1,help='number of STAtt Blocks')\n",
    "parser.add_argument('--K', type=int, default=8,help='number of attention heads')\n",
    "parser.add_argument('--d', type=int, default=8,help='dims of each head attention outputs')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7,help='training set [default : 0.7]')\n",
    "parser.add_argument('--val_ratio', type=float, default=0.1,help='validation set [default : 0.1]')\n",
    "parser.add_argument('--test_ratio', type=float, default=0.2,help='testing set [default : 0.2]')\n",
    "parser.add_argument('--batch_size', type=int, default=256,help='batch size')\n",
    "parser.add_argument('--max_epoch', type=int, default=1,help='epoch to run')\n",
    "parser.add_argument('--patience', type=int, default=10,help='patience for early stop')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,help='initial learning rate')\n",
    "parser.add_argument('--decay_epoch', type=int, default=10,help='decay epoch')\n",
    "parser.add_argument('--traffic_file', default='./data/pems-bay.h5',help='traffic file')\n",
    "parser.add_argument('--SE_file', default='./data/SE(PeMS).txt',help='spatial embedding file')\n",
    "parser.add_argument('--model_file', default='./data/GMAN.pkl',help='save the model to disk')\n",
    "parser.add_argument('--log_file', default='./data/log',help='log file')\n",
    "args = parser.parse_args(args=[])\n",
    "log = open(args.log_file, 'w')\n",
    "log_string(log, str(args)[10: -1])\n",
    "T = 24 * 60 // args.time_slot  # Number of time steps in one day\n",
    "# load data\n",
    "log_string(log, 'loading data...')\n",
    "(trainX, trainTE, trainY, valX, valTE, valY, testX, testTE,\n",
    " testY, SE, mean, std) = load_data(args)\n",
    "log_string(log, f'trainX: {trainX.shape}\\t\\t trainY: {trainY.shape}')\n",
    "log_string(log, f'valX:   {valX.shape}\\t\\tvalY:   {valY.shape}')\n",
    "log_string(log, f'testX:   {testX.shape}\\t\\ttestY:   {testY.shape}')\n",
    "log_string(log, f'mean:   {mean:.4f}\\t\\tstd:   {std:.4f}')\n",
    "log_string(log, 'data loaded!')\n",
    "del trainX, trainTE, valX, valTE, testX, testTE, mean, std\n",
    "# build model\n",
    "log_string(log, 'compiling model...')\n",
    "\n",
    "model = GMAN(SE, args, bn_decay=0.1)\n",
    "loss_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=args.decay_epoch,gamma=0.9)\n",
    "parameters = count_parameters(model)\n",
    "log_string(log, 'trainable parameters: {:,}'.format(parameters))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    loss_train, loss_val = train(model, args, log, loss_criterion, optimizer, scheduler)\n",
    "    plot_train_val_loss(loss_train, loss_val, 'figure/train_val_loss.png')\n",
    "    trainPred, valPred, testPred = test(args, log)\n",
    "    end = time.time()\n",
    "    log_string(log, 'total time: %.1fmin' % ((end - start) / 60))\n",
    "    log.close()\n",
    "    trainPred_ = trainPred.numpy().reshape(-1, trainY.shape[-1])\n",
    "    trainY_ = trainY.numpy().reshape(-1, trainY.shape[-1])\n",
    "    valPred_ = valPred.numpy().reshape(-1, valY.shape[-1])\n",
    "    valY_ = valY.numpy().reshape(-1, valY.shape[-1])\n",
    "    testPred_ = testPred.numpy().reshape(-1, testY.shape[-1])\n",
    "    testY_ = testY.numpy().reshape(-1, testY.shape[-1])\n",
    "\n",
    "    # Save training, validation and testing datas to disk\n",
    "    l = [trainPred_, trainY_, valPred_, valY_, testPred_, testY_]\n",
    "    name = ['trainPred', 'trainY', 'valPred', 'valY', 'testPred', 'testY']\n",
    "    for i, data in enumerate(l):\n",
    "        np.savetxt('./figure/' + name[i] + '.txt', data, fmt='%s')\n",
    "        \n",
    "    # Plot the test prediction vs target锛坥ptional)\n",
    "    plt.figure(figsize=(10, 280))\n",
    "    for k in range(325):\n",
    "        plt.subplot(325, 1, k + 1)\n",
    "        for j in range(len(testPred)):\n",
    "            c, d = [], []\n",
    "            for i in range(12):\n",
    "                c.append(testPred[j, i, k])\n",
    "                d.append(testY[j, i, k])\n",
    "            plt.plot(range(1 + j, 12 + 1 + j), c, c='b')\n",
    "            plt.plot(range(1 + j, 12 + 1 + j), d, c='r')\n",
    "    plt.title('Test prediction vs Target')\n",
    "    plt.savefig('./figure/test_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23ceb7112fbf9d0e38ecbf60d6e6d5e2dcebcc82200eeb1e5a5d5f9ffb9e27ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
